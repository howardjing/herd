\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Scientific Computing Final Project}
\author{Howard Jing and Sun Hyoung Sonya Kim}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
\section*{Power Iteration}
The power iteration algorithm is a method of computing the dominant eigenvalue of a matrix $A$ $\in \mathbb{C}^{n \times n}$. Assuming that $A$ has n distinct eigenvalues, it can be factorized in the form of $A = V \Lambda V^{-1}$, where $\Lambda$ is a diagonal matrix with eigenvalues $\lambda_1 ,..., \lambda_n$. Furthermore, assume that the eigenvalues are ordered from largest to smallest, $|\lambda_1| > |\lambda_2| > ... > |\lambda_n|$, meaning that $\lambda_1$ is the largest eigenvalue, while $\lambda_n$ is the smallest eigenvalue.

Note that: 
\begin{align*}
	&A = V\Lambda V^{-1} \\
	\implies &A^k = (V\Lambda V^{-1})(V\Lambda V^{-1})...(V\Lambda V^{-1}) = V\Lambda ^k V^{-1} \\	
	\implies &A^kV = V\Lambda ^k
\end{align*}
Suppose we have a vector $x = V \hat{x} $ $\in$ $\mathbb{C}^{n \times n}$, then because $\Lambda$ is a diagonal matrix, we have that:
\begin{align*}
	A^kx &= A^kV\hat{x} \\
	& = (V\Lambda ^k)\hat{x}  \\
	& = \Sigma^{n}_{i=1} v_i \lambda_i \hat{x}_i \\
\end{align*}
Pulling out the dominant eigenvalue $\lambda_1^k$, we have that:
\begin{align*}
	A^kx = \lambda_1^k(\sum_{i=1}^n v_i \frac{\lambda_i}{\lambda_1}^k \hat{x}_i)
\end{align*}
and since for $i > 1$ we know that $|\frac{\lambda_i}{\lambda_1}| < 1$, we see that $(\frac{\lambda_i}{\lambda_1})^k \rightarrow 0$ as $k$ grows large.

This implies that as $k$ grows large, 
\[ 
	A^k x \approx \lambda_1^k v_1 \hat{x}_1
\]
which implies the power iteration algorithm,
\[
	x_{k+1} = \frac{Ax_k}{||Ax_k||}
\]
since
\[
	\frac{Ax_k}{||Ax_k||} = \frac{A^kx_0}{||A^kx_0||}
\]
This can be seen by induction.

The base case is true by definition:
\[
	x_2 = \frac{Ax_0}{||Ax_0||} = \frac{A^1x_0}{||A^1x_0||}
\]
The induction hypothesis is then:
\[
	x_{k+1} = \frac{Ax_k}{||Ax_k||} = \frac{A^kx_0}{||A^kx_0||}
\]
Then by applying the algorithm, we get that:
\begin{align*}
	x_{k+2} = \frac{Ax_{k+1}}{||Ax_{k+1}||} &= \frac{A(\frac{A^kx_0}{||A^kx_0||})}{||A(\frac{A^kx_0}{||A^kx_0||})||}\\
	&= \frac{1}{||A^kx_0||}A^{k+1}x_0(||A^{k+1}x_0||\frac{1}{||A^kx_0||})^{-1} \\
	&=\frac{A^{k+1}x_0}{||A^{k+1}x_0||}
\end{align*}
So the formula is proved. 

As k increases, the algorithm will converge towards the eigenvector associated with the matrix $A$'s dominant eigenvalue. Note that if $\frac{\lambda_i}{\lambda_1}$ is near one, then the algorithm will converge very slowly. However, if $\lambda_1$ is much larger than the rest of the eigenvalues, then the algorithm will converge at a reasonable rate. 

At each iteration step of the power iteration algorithm,  a matrix-vector multiplication is carried out, which implies an operation count of $O(n^2)$ operations. Moreover, the matrix A must be accessed once at each iteration, for a total of n times.

\section*{Inverse Iteration}
If an invertible $n\times n$ matrix A has eigenvalues $\lambda_1,...,\lambda_n$ , then its inverse $A^{-1}$ has eigenvalues $\frac{1}{\lambda_1}$,..,$\frac{1}{\lambda_n}$. If we run the power iteration algorithm on the matrix $A^{-1}$, we can then find the smallest eigenvalue $\lambda_n$ of the matrix A. This process can in fact be generalized to find any eigenvalue of the matrix A.

The spectral mapping theorem states that for every $n\times n$ matrix A and every polynomial $p(x)$, if A has an eigenvalue $\lambda_i$, then $p(A)$ has an eigenvalue $p(\lambda_i)$. Then the matrix $B := A - \sigma I$ has eigenvalues of $\{\lambda_1 - \sigma,...,\lambda_n - \sigma\}$. If $\sigma$ is chosen to be very close to the arbitrary eigenvalue $\lambda_i$, then the corresponding eigenvalue $\lambda_i - \sigma$ will be the smallest eigenvalue of the matrix $B$.

By applying the power iteration algorithm on the matrix $B^{-1}$, we thus arrive at the inverse iteration algorithm: 
\[
	x_{k+1} = \frac{(A-\sigma I)^{-1}x_k}{||(A-\sigma I)^{-1}x_k||}
\]
which converges to the eigenvalue of $A$ that is closest to the given estimated eigenvalue $\sigma$. Unlike the power iteration algorithm, which only computes the dominant eigenvalue of the matrix $A$, the inverse iteration algorithm is more flexible as it can be used to compute an arbitrary eigenvalue of the matrix $A$.

However the inverse iteration method is more costly than the power iteration method. This is because at each iteration step, either a system of linear equations must be solved, or the inverse matrix must be calculated. This implies that each step of the inverse iteration algorithm costs $O(n^3)$ operations. Similar to the power iteration algorithm, the matrix A must be accessed once at each iteration, for a total of of n times. 
\end{document}  